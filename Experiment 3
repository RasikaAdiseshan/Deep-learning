import numpy as np
X=np.array([[2,9],[1,5],[3,6]],dtype=float)
y=np.array([[92],[86],[89]],dtype=float)
X=X/np.amax(X,axis=0)
y=y/100
print(X)
print(y)
def sigmoid(x):
  return 1/(1+np.exp(-x))

def derivatives_sigmoid(x):
  return x*(1-x)

epoch=5
lr=0.1
inputlayer_neurons=2
hiddenlayer_neurons=3
output_neurons=1
#Initializing weights and bias
wt=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
b=np.random.uniform(size=(1,hiddenlayer_neurons))
wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))

for i in range(epoch):
  #Forward Propogation
  hinp1=np.dot(X,wt)
  hinp=hinp1+b
  hlayer_act=sigmoid(hinp)
  outinp1=np.dot(hlayer_act,wout)
  outinp=outinp1+bout
  output=sigmoid(outinp)
#Backpropogation
EO=y-output
outgrad=derivatives_sigmoid(output)
d_output=EO*outgrad
EH=d_output.dot(wout.T)
hiddengrad=derivatives_sigmoid(hlayer_act)
d_hiddenlayer=EH*hiddengrad
#Updating weights and biases
wout+=hlayer_act.T.dot(d_output)*lr
wt+=X.T.dot(d_output)*lr
print("--------Epoch--",i+1,"Starts------")
print("Input:\n",X)
print("Actual Output:\n",y)
print("Predicted Output: \n", output)
print("---------Epoch-", i + 1, "Ends---------\n")

print("Input: \n", X)
print("Actual Output: \n", y)
print("Predicted Output: \n", output)
